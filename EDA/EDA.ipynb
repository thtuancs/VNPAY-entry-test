{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other modules not related to PySpark\n",
    "from datetime import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sweetviz as sv\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import statistics as stats\n",
    "# This helps auto print out the items without explixitly using 'print'\n",
    "InteractiveShell.ast_node_interactivity = \"all\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lit, desc, col, size, array_contains\\\n",
    ", isnan, udf, hour, array_min, array_max, countDistinct, to_date, to_timestamp\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = '15G'\n",
    "# Initialize a spark session.\n",
    "conf = pyspark.SparkConf().setMaster(\"local[*]\") \\\n",
    "        .set('spark.executor.heartbeatInterval', 10000) \\\n",
    "        .set('spark.network.timeout', 10000) \\\n",
    "        .set(\"spark.core.connection.ack.wait.timeout\", \"3600\") \\\n",
    "        .set(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "        .set(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "        .set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "        \n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"transaction_data\") \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()\n",
    "filename_data = './transaction_data.csv'\n",
    "df = spark.read.csv(filename_data, inferSchema=True, header=True)\n",
    "\n",
    "print('Data overview')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data frame describe (string and numeric columns only)')\n",
    "df.describe().toPandas()\n",
    "\n",
    "print(f'Total {df.count()} rows')\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Clean data and filter Outliers (Explain your ideas and ways how to find out Outliers and the ways to treat them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill -1 with None\n",
    "df = df.na.replace(to_replace={-1: None})\n",
    "print(\"df length before cleaning: \", df.count())\n",
    "\n",
    "# drop duplicate\n",
    "df = df.drop_duplicates()\n",
    "print(\"df length after drop duplicates: \", df.count())\n",
    "\n",
    "# drop NA\n",
    "df = df.na.drop()\n",
    "print(\"df length after drop na: \", df.count())\n",
    "\n",
    "# drop NumberOfItemsPurchased <= 0\n",
    "df = df.filter(col(\"NumberOfItemsPurchased\") > 0)\n",
    "print(\"df length after drop non-positive NumberOfItemsPurchased: \", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Same TransactionId and ItemCode but different CostPerItem\")\n",
    "\n",
    "df.select(['TransactionId', 'ItemCode', 'CostPerItem'])\\\n",
    "    .drop_duplicates()\\\n",
    "    .groupBy(['TransactionId', 'ItemCode'])\\\n",
    "    .count()\\\n",
    "    .filter(col('count') > 1)\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UserId, TransactionId, ItemCode are category.\n",
    "\n",
    "# Calculate Z-score for CostPerItem and NumberOfItemsPurchased\n",
    "\n",
    "for column in ['CostPerItem', 'NumberOfItemsPurchased']:\n",
    "    stats = df.select(functions.mean(col(column)).alias('mean'), functions.stddev(col(column)).alias('stddev')).collect()[0]\n",
    "    df = df.withColumn(f'{column}_z_score', (col(column) - stats['mean']) / stats['stddev'])\n",
    "\n",
    "outlier_candidate_df = df.filter('abs(CostPerItem_z_score) > 3').filter('abs(NumberOfItemsPurchased_z_score) > 3')\n",
    "\n",
    "# Some of rows that have Z-score > 3, but it's seem to be normal because price and quantity can be arbitrary.\n",
    "\n",
    "df = df.drop('CostPerItem_z_score', 'NumberOfItemsPurchased_z_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Calculate the number of Items purchased and prices in each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort('TransactionTime', ascending=True)\n",
    "\n",
    "df = df.withColumn('date', to_date(df['TransactionTime'], 'E MMM dd HH:mm:ss zzz yyyy').cast(DateType()).cast('timestamp'))\n",
    "\n",
    "# found out that some date belong to 2028, which is not valid date. So I will filter them out.\n",
    "df = df.filter(col('date') < functions.current_date())\n",
    "\n",
    "df = df.withColumn('TotalItemCost', col('NumberOfItemsPurchased') * col('CostPerItem'))\n",
    "\n",
    "month_grouped_df = df.groupBy(functions.month('date').alias('month'), functions.year('date').alias('year'))\n",
    "result_df = month_grouped_df.agg(\n",
    "    functions.sum('NumberOfItemsPurchased').alias('NumberOfItemsPurchased_by_month'),\n",
    "    functions.sum('TotalItemCost').alias('TotalItemCost_by_month'),\n",
    ").sort('year', 'month')\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Calculate the number of items purchased for each userID in 30 days for each day (Add new column) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = lambda i: i * 86400\n",
    "\n",
    "grouped_df = df.groupBy('UserId', 'date').agg(functions.sum('NumberOfItemsPurchased').alias('NumberOfItemsPurchased'))\n",
    "\n",
    "w = Window.partitionBy('UserId').orderBy(F.col(\"date\").cast('long')).rangeBetween(-days(30), 0)\n",
    "result_df = grouped_df.withColumn('NumberOfItemsPurchased_30days', functions.sum(\"NumberOfItemsPurchased\").over(w))\n",
    "\n",
    "result_df.sort('UserId', 'date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas().to_csv('./clean_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
